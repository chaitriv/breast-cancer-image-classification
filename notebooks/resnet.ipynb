{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## Setup Kaggle & Download Dataset\n",
        "\n",
        "from google.colab import files\n",
        "import os, shutil, glob\n",
        "\n",
        "files.upload()  # Upload kaggle.json\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 600)\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"ambarish/breakhis\")\n",
        "\n",
        "# Copy dataset to content folder\n",
        "dataset_images_path = os.path.join(path, \"BreaKHis_v1\", \"BreaKHis_v1\", \"histology_slides\", \"breast\")\n",
        "destination = \"/content/breakhis_dataset\"\n",
        "if not os.path.exists(destination):\n",
        "    shutil.copytree(dataset_images_path, destination)\n",
        "\n",
        "# Dataset overview\n",
        "benign_images = glob.glob(os.path.join(destination, \"benign\", \"**\", \"*.png\"), recursive=True)\n",
        "malignant_images = glob.glob(os.path.join(destination, \"malignant\", \"**\", \"*.png\"), recursive=True)\n",
        "magnifications = [\"40X\", \"100X\", \"200X\", \"400X\"]\n",
        "image_counts = {mag: len(glob.glob(os.path.join(destination, \"**\", mag, \"*.png\"), recursive=True)) for mag in magnifications}\n",
        "\n",
        "\n",
        "## Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=[\"Benign\", \"Malignant\"], y=[len(benign_images), len(malignant_images)])\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=list(image_counts.keys()), y=list(image_counts.values()))\n",
        "plt.title(\"Image Count by Magnification\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.xlabel(\"Magnification\")\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
        "for i, mag in enumerate(magnifications):\n",
        "    benign_sample = glob.glob(os.path.join(destination, \"benign\", \"**\", mag, \"*.png\"), recursive=True)[0]\n",
        "    malignant_sample = glob.glob(os.path.join(destination, \"malignant\", \"**\", mag, \"*.png\"), recursive=True)[0]\n",
        "    axes[0, i].imshow(Image.open(benign_sample))\n",
        "    axes[0, i].set_title(f\"Benign - {mag}\")\n",
        "    axes[0, i].axis(\"off\")\n",
        "    axes[1, i].imshow(Image.open(malignant_sample))\n",
        "    axes[1, i].set_title(f\"Malignant - {mag}\")\n",
        "    axes[1, i].axis(\"off\")\n",
        "plt.suptitle(\"Sample Images by Class and Magnification\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "IMG_SIZE = 224\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "data = []\n",
        "for class_name in [\"benign\", \"malignant\"]:\n",
        "    class_path = os.path.join(dataset, class_name, \"SOB\")\n",
        "    for subtype in os.listdir(class_path):\n",
        "        subtype_path = os.path.join(class_path, subtype)\n",
        "        for sample in os.listdir(subtype_path):\n",
        "            for mag in magnifications:\n",
        "                mag_path = os.path.join(subtype_path, sample, mag)\n",
        "                for img_path in glob.glob(os.path.join(mag_path, \"*.png\")):\n",
        "                    data.append([img_path, class_name, mag])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"image_path\", \"label\", \"magnification\"])\n",
        "df[\"label\"] = df[\"label\"].map({\"benign\": 0, \"malignant\": 1})\n",
        "\n",
        "class BreakHisDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.dataframe.iloc[idx][\"image_path\"]\n",
        "        label = self.dataframe.iloc[idx][\"label\"]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "## Train-Test Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
        "train_loader = DataLoader(BreakHisDataset(train_df, transform=transform), batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(BreakHisDataset(test_df, transform=transform), batch_size=32)\n",
        "\n",
        "## Train ResNet18\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
        "model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "            outputs = model(images).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "    print(f\"Validation Loss: {val_loss / len(test_loader):.4f}\")\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "## Evaluate & Save Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()\n",
        "predictions, true_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "        outputs = model(images).squeeze()\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "acc = accuracy_score(true_labels, predictions)\n",
        "print(f\"Test Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "torch.save(model.state_dict(), \"breakhis_resnet18.pth\")\n",
        "\n",
        "\n",
        "## Hyperparameter Tuning with Optuna\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adam\", \"sgd\"])\n",
        "    model = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
        "    model.fc = nn.Linear(model.fc.in_features, 1)\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) if optimizer_name == \"adam\" else optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device, dtype=torch.float32)\n",
        "            outputs = model(images).squeeze()\n",
        "            preds = torch.sigmoid(outputs) > 0.5\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    return accuracy_score(true_labels, predictions)\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "print(\"Best parameters:\", study.best_params)\n"
      ],
      "metadata": {
        "id": "osbpTMGPJGRm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}